{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É um processo de extração de informações, que começa com os _Spiders_ que são programas automatizados que estão o tempo todo extraindo informações dos sites, estruturando-as e armazenando em grandes bases de dados, que por sua vez são consultados por ferramentas de buscas. ___(Processo basico q ilustra como o google se mantem atualizado)___ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma ferramenta em python de codigo aberto, interface simples de fácil utilização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Instalando o Scrapy__ <br/>\n",
    "_pip install scrapy_ <br/>\n",
    "<br/>\n",
    "__Criando um projeto__  <br/>\n",
    "_scrapy startproject nome_do_projeto_   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Estrutura de um projeto no Scrapy__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_scrapy.cfg_   ----->  Configurações do deploy <br/>\n",
    "_items.py_   -----> Modelo (Abstração da informação que quero extrair) <br/>\n",
    "_pipelines.py_ -----> Classes que sao executadas em sequencia ao spider, por meio deles podemos configurar o scrapy para armazenar em um banco dados as informações extraidas automaticamente <br/>\n",
    "_settings.py_ -----> Configurações gerais do projeto\n",
    "_mySpider.py_ -----> Contem os spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Começando um Projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente criamos nosso modelo,que é uma simples classe em python, onde definimos quais informações buscaremos na url ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida criamos nosso Spider (tb uma classe em pyhton), que herda uma classe chamada CrawlSpider do proprio Scrapy. <br/>\n",
    "é obrigatorio definir o primeiro metodo dessa classe, o metodo parse, tal metodo é o responsável por lidar com o html que será retornado da url para retirar as informações desejadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Para tratar este html utilizaremos uma linguagem de navegação em documentos html chamada Xpath_ .<br/>\n",
    "Uma forma simples de usar o Xpath com o Scrapy é através do Scrapy Shell: um console interativo que permite testar os comandos em Xpath na propria pagina de interesse.<br/>\n",
    "<br/>\n",
    "Para inicia-lo basta, no terminal, _scrapy shell url_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma forma de aplicar o Xpath no retorno html da url, é através da estutura de dados do proprio scrapy, chamada ___Seletores___ que encapsula a html retornada e aplica o xpath nele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
