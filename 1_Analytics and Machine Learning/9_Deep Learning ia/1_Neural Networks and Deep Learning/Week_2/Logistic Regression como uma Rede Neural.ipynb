{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação Binária"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De maneira geral uma classificação binária é: Dada uma entrada __X__ prever se label __Y__ correspondente é __0__ ou __1.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Notação:___\n",
    "\n",
    "* Par unico de treino: <br>\n",
    "    * $(x, y)$ $|$ $x \\in \\Re^{n_x}, y \\in \\{0,1\\}$\n",
    "\n",
    "* Amostras de treinos:\n",
    "    * $m: [(x^1, y^1), (x^2, y^2)...(x^m, y^m)]$\n",
    "\n",
    "* Train/Test:\n",
    "    * Conjunto de treino: $m_{treino}$\n",
    "    * Conjunto de teste: $m_{teste}$\n",
    "    \n",
    "* Matriz de input\n",
    "    * Matriz contendo em cada coluna os valores de cada feature\n",
    "    * $X \\in \\Re^{n_x,m}$\n",
    "\n",
    "* Matriz de saida\n",
    "    * Matriz contendo as labels de saida\n",
    "    * $Y \\in \\Re^{1, m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logistica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado $x$ queremos prever a saída $ŷ$, tal que $ŷ = P(y=1|x)$\n",
    "\n",
    "Em que $x \\in \\Re^{n_x}$, e numa regressão logística temos: $w \\in \\Re^{n_x}, b \\in \\Re$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para prever uma saída $ŷ$ poderiamos usar uma função linear dos parametros para chegar na saída:\n",
    "\n",
    "$ŷ = w^Tx + b$\n",
    "\n",
    "Porém neste caso $ŷ$ poderá assumir valores muito maiores que 1 e até mesmo negativo, o que não faz sentido se estamos interessados na probabilidade de saída. Sendo assim aplicamos a função ___sigmoide___ a esta equação:\n",
    "\n",
    "\\begin{equation*}\n",
    "    ŷ = \\sigma(w^Tx + b)\n",
    "\\end{equation*}\n",
    "\n",
    "Onde $\\sigma(z) = \\frac{1}{1 + e^{-z}}, z \\in \\Re$\n",
    "\n",
    "E fazendo $z = w^Tx + b$ temos:\n",
    "\n",
    "\\begin{equation*}\n",
    "    ŷ = \\sigma(z)\n",
    "\\end{equation*}\n",
    "\n",
    "Portanto:\n",
    "* Se $z \\rightarrow +\\infty$ então $\\sigma(z) \\approx \\frac{1}{1 + 0} = 1$\n",
    "\n",
    "\n",
    "* Se $z \\rightarrow -\\infty$ então $\\sigma(z) \\approx \\frac{1}{1 + \\infty} = 0$\n",
    "\n",
    "\n",
    "Portanto, ao treinar uma regressão logística, o trabalho é aprender os parametros $w$ e $b$ de forma que $ŷ$ seja uma boa estimativa da chance de $y$ ser igual a $1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL - Função Custo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que temos como entrada $\\{(x^1, y^1)...(x^m, y^m)\\}$, e queremos $ŷ^i \\approx y^i$. Então para cada $ŷ^i$ teremos:\n",
    "\n",
    "$ŷ^i = \\sigma(w^Tx^i + b),$ onde $\\sigma(z^i) = \\frac{1}{1 + e^{-z^i}}$ e $z^i = w^Tx^i + b$\n",
    "\n",
    "\n",
    "_OBS: o índice \"$i$\" refere-se aos dados de treinamento_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos determinar o quanto o modelo desempenha bem, avaliando a sua perda, ou seja, quando o algortimo da o resulta $ŷ$ em relação a label verdadeira $y$ - __Loss (error) Function.__\n",
    "\n",
    "\n",
    "Poderiamos definir a Loss function como a metade do erro quadrado médio:\n",
    "$L(ŷ, y) = \\frac{1}{2}(ŷ - y)^2$\n",
    "\n",
    "Porém em casos onde conhecemos as labels, teremos uma Loss function com flutuações, ou seja com muitos _minimos locais_ e sendo assim podemos ter um gradiente que nunca encontre um _minimo global._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portanto em Regressão Logística a Loss Function usada é:\n",
    "\n",
    "$L(ŷ, y) = -(ylogŷ + (1 - y) log(1 - ŷ))$\n",
    "\n",
    "Pois a idéia é que esta função assuma o __menor valor possível__ (minimo global)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Exemplo:___\n",
    "\n",
    "Se $y = 1$ : $L(ŷ, y) = -logŷ$ $\\rightarrow$ $logŷ$ deve ser o maior possível então $ŷ \\rightarrow \\infty$\n",
    "\n",
    "Se $y = 0$ : $L(ŷ, y) = -log(1 - ŷ)$ $\\rightarrow$ $log(1 - ŷ)$ deve ser o maior possível então $ŷ \\rightarrow -\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Loss Function avalia como o modelo desempenha sobre um exemplo treino, queremos então definir uma função que nos dê essa estimativa sobre todo o conjunto de treino: __Cost Function__.\n",
    "\n",
    "Logo, a cost function avalia o desempenho de todo o conjunto de treino, dado os parametros $w$ e $b.$ \n",
    "\n",
    "\\begin{equation*}\n",
    "J(w, b) = \\frac{1}{m} \\sum^{m}_{i=1}L(ŷ^i, y^i) = -\\frac{1}{m}\\sum^{m}_{i=1}[y^ilogŷ^i + (1 - y^i) log(1 - ŷ^i)]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto ao treinar uma Regressão Logística queremos determinar os parâmetros $w$ e $b$ que minimizam a função custo total $J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](gd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gradiente Descent__ é um metodo de otimização para alcançar o minimo global. Através de iterações repetidas, realizando derivadas e encontrando assim a menor direção para a convergência, os parametros ( _w e b_ ) são atualizados até que se chegue ao minimo global."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "    w := w - \\alpha \\frac{dJ(w)}{dw}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onde $\\alpha$ é o __Learning Rate__, e determina o quão grande será o proximo passo tomado a cada iteração do gradiente decrescente.\n",
    "\n",
    "OBS: Convenciona-se que $\\frac{dJ(w)}{dw} := dw$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__OBS1:__ ___O conceito de \"Back Propagation\" refere-se a regra da cadeia___\n",
    "\n",
    "__OBS2:__ ___O resultado final da regra da cadeira convenciona-se que será___ $dvar$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent in Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para todo o conjunto de treino teremos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{d}{dw_1} J(w, b) = \\frac{1}{m} \\sum^{m}_{i=1} \\frac{d}{dw_i} L(a^i, y^i) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vetorização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza-se __vetorização__ para evitar os \" _loops_ \" no código e assim obter mais eficiência no programa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seja, numa regressão logistica $z = w^T + b$, onde $w$ e $x$, com $w \\in \\Re^{n_x}$ e $x \\in \\Re^{n_x}$ são \"vetores colunas\".\n",
    "\n",
    "__Exemplo:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1,2,3,4])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250176.34278775906\n",
      "Vectorizer version: 1.5251636505126953ms\n",
      "\n",
      "250176.3427877512\n",
      "For loop: 696.0258483886719ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "\n",
    "tic = time.time()\n",
    "c = np.dot(a, b)\n",
    "toc = time.time()\n",
    "\n",
    "print(c)\n",
    "print(\"Vectorizer version: \" + str(1000*(toc-tic)) + \"ms\")\n",
    "print()\n",
    "\n",
    "c = 0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i]*b[i]\n",
    "toc = time.time()\n",
    "\n",
    "print(c)\n",
    "print(\"For loop: \" + str(1000*(toc-tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mais Exemplos:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se fossemos implementar o Grandiente Descendente sem usar vetorização teriamos algo como:\n",
    "\n",
    "$J = 0$, $dw_1 = 0$, $dw_2 = 0$, $db = 0$<br>\n",
    "for i=1 to m: <br>\n",
    "$z^i = w^T x^i + b$ <br>\n",
    "$a^i = \\sigma(z^i)$ <br>\n",
    "$J += -[y^ilogŷ^i + (1 - y^i)log(1 - ŷ^i)]$ <br>\n",
    "$dz^i = a^i(1 - a^i)$ <br>\n",
    "$dw_1 = x_1^i dz^i$ <br>\n",
    "$dw_2 = x_2^i dz^i$ <br>\n",
    "$db += dz^i$ <br>\n",
    "<br>\n",
    "$J = J/m$, $dw_1 = dw_1/m$, $sw_2 = dw_2/m$, $db = db/m$\n",
    "\n",
    "\n",
    "Observando que neste caso temos duas _features_ , porem a medida que as features aumentam o algoritmo tb aumentaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora implementando com vetorização seria:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J = 0$, $dw = np.zeros((nx, 1))$, $db = 0$<br>\n",
    "for i=1 to m: <br>\n",
    "$z^i = w^T x^i + b$ <br>\n",
    "$a^i = \\sigma(z^i)$ <br>\n",
    "$J += -[y^ilogŷ^i + (1 - y^i)log(1 - ŷ^i)]$ <br>\n",
    "$dz^i = a^i(1 - a^i)$ <br>\n",
    "$dw += x^i dz^i$ <br>\n",
    "$db += dz^i$ <br>\n",
    "<br>\n",
    "$J = J/m$, $dw = dw/m$, $db = db/m$\n",
    "\n",
    "OBS: Desta maneira teremos apenas _1 loop_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vetorizando uma Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrada do gradiente descendente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numa RL precisamos utilizar a regra da cadeia sobre cada exemplo de treino para realizar uma previsão, vejamos como podemos implementar isso sem utilizar _loops._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seja $X = [x^1, x^2, ..., x^m]$ uma matriz $(n_x, m)$, queremos calcular $z^i = w^Tx^i + b$ para todos os elementos do conjunto de treino, sendo assim, ao final dos calculos teriamos um vetor de $z_s$, ou seja: <br>\n",
    "$[z^1, z^2, ..., z^m]$\n",
    "\n",
    "\n",
    "Então, teriamos que: <br>\n",
    "$[z^1, z^2, ..., z^m] = w^T X + [b,b, ..., b] = [(w^Tx^1 + b), (w^Tx^2 + b), ..., (w^Tx^m + b)]$\n",
    "\n",
    "Vemos então que: <br>\n",
    "$z^1 = w^Tx^1 + b$, $z^2 = w^Tx^2 + b$, .... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portanto, em Python podemos simplesmente fazer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.dot(w.T, X) + b\n",
    "\n",
    "\"\"\"\n",
    "w.T --> vetor transposto\n",
    "+ b --> Python broadcasting: expande b em um vetor (1, m) e aplica a soma sobre o vetor anterior\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O mesmo vale para o que seria a próxima etapa da __back propagation__ (aplicação da regra da cadeia) que é: <br>\n",
    "$a^1 = \\sigma(z^1)$, $a^2 = \\sigma(z^2)$, ..., $a^m = \\sigma(z^m)$\n",
    "\n",
    "Teremos ao final: <br>\n",
    "$[a^1, a^2, ..., a^m]$\n",
    "\n",
    "Portanto bastaria fazermos: <br>\n",
    "$A = [a^1, a^2, ..., a^m] = \\sigma(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saída do gradiente descendente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos que $A = [a^1, a^2, ..., a^m]$ e como saida do modelo, as previsões, teriamos $Y = [y^1, y^2, ..., y^m]$\n",
    "\n",
    "\n",
    "Como visto anteriormente a derivada de $z$, podemos então verificar que: <br>\n",
    "$dZ = A - Y = [a^1 - y^1, a^2 - y^2, ..., a^m - y^m]$\n",
    "\n",
    "\n",
    "Portanto temos:<br> \n",
    "$db = \\frac{1}{m} \\sum^m_{i=1} dz^i$ <br>\n",
    "$dw = \\frac{1}{m} X dZ^T = \\frac{1}{m} [x^1, x^2, ..., x^m][dz^1, dz^2, ..., dz^m] = \\frac{1}{m} [x^1dz^1, x^2dz^2, ..., x^mdz^m]$\n",
    "\n",
    "Sendo assim, em Python temos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = 1/m * np.sum(dZ)\n",
    "\n",
    "dw = 1/m * X * dZ.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Juntando tudo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para uma implementação vetorizada da regressão logística, temos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Z = w^TX + b$ <br>\n",
    "$A = \\sigma(Z)$ <br>\n",
    "$dz = A - Y$ <br>\n",
    "$dw = \\frac{1}{m} X dZ^T$ <br>\n",
    "$db = \\frac{1}{m} np.sum(dZ)$ <br>\n",
    "\n",
    "Assim temos o calculo para back propagation e retro-propagation, que são simplesmente as etapadas da regra da cadeia.\n",
    "\n",
    "Para a atualização dos pesos:\n",
    "\n",
    "$w := w - \\alpha dw$ <br>\n",
    "$b := b - \\alpha db$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python/numpy Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11372434  1.31904447 -1.39430473 -0.54826907  1.51755754]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.randn(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "# Ao printar a vetor criado acima, vemos que ele não é do tipo \"Vetor linha\" nem do tipo \"Vetor Coluna\"\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal estrutura de dado acima (5,) chama-se \" __rank 1 array__ \", e possui comportamentos esquisitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11372434  1.31904447 -1.39430473 -0.54826907  1.51755754]\n"
     ]
    }
   ],
   "source": [
    "# Isso pode levar a alguns erros de interpretação\n",
    "# Transposta\n",
    "print(a.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos então imaginar que multiplicando a por sua transposta teremos uma matriz simétrica, ou outra matriz, porem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.300477082523405\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(a, a.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Tip: Não utilizar estruturas da forma (n,) sempre \"forçar\" a dimensão da estrutura, (n,1)___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.82388622]\n",
      " [0.36344221]\n",
      " [0.04575223]\n",
      " [0.67015828]\n",
      " [0.18260092]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(5,1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.82388622 0.36344221 0.04575223 0.67015828 0.18260092]]\n"
     ]
    }
   ],
   "source": [
    "print(a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6787885  0.29943503 0.03769463 0.55213417 0.15044238]\n",
      " [0.29943503 0.13209024 0.01662829 0.24356381 0.06636488]\n",
      " [0.03769463 0.01662829 0.00209327 0.03066124 0.0083544 ]\n",
      " [0.55213417 0.24356381 0.03066124 0.44911212 0.12237152]\n",
      " [0.15044238 0.06636488 0.0083544  0.12237152 0.03334309]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(a, a.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.30814041, -0.24600646,  1.760964  ],\n",
       "       [ 0.56626059, -0.09426146, -0.6328777 ],\n",
       "       [-0.26841861,  0.58378288,  0.21296354]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(3, 3)\n",
    "b = np.random.randn(3, 1)\n",
    "c = a*b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
